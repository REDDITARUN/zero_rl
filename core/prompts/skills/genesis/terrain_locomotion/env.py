"""Terrain Locomotion: quadruped walking over mixed terrain with mesh obstacles.

Demonstrates:
- gs.morphs.Terrain for procedural heightfield terrain (multiple sub-terrain types)
- gs.morphs.Mesh for loading custom STL assets as static obstacles
- Combining URDF robot + terrain + mesh props in one scene
- All entities added BEFORE scene.build()

The robot navigates varied terrain (slopes, stairs, stepping stones) while
avoiding tree and rock obstacles loaded from STL files generated by cad_generate.
"""

from __future__ import annotations

import math
from pathlib import Path
from typing import Any, Dict

import torch

import genesis as gs
from genesis.utils.geom import (
    inv_quat,
    transform_by_quat,
)

from .rewards import REWARD_REGISTRY


class TerrainLocomotionEnv:
    """Parallel locomotion environment with terrain and mesh obstacles.

    Scene composition:
        1. gs.morphs.Terrain — procedural sub-terrain grid
        2. gs.morphs.URDF    — Go2 quadruped robot
        3. gs.morphs.Mesh    — STL obstacles (rocks, trees) from assets/
    """

    def __init__(
        self,
        num_envs: int,
        env_cfg: Dict[str, Any],
        obs_cfg: Dict[str, Any],
        reward_cfg: Dict[str, Any],
        command_cfg: Dict[str, Any],
        show_viewer: bool = False,
    ) -> None:
        self.num_envs = num_envs
        self.num_obs = obs_cfg["num_obs"]
        self.num_actions = env_cfg["num_actions"]
        self.num_commands = command_cfg["num_commands"]
        self.device = gs.device
        self.dt = env_cfg["dt"]
        self.max_episode_length = math.ceil(env_cfg["episode_length_s"] / self.dt)

        self.env_cfg = env_cfg
        self.obs_cfg = obs_cfg
        self.reward_cfg = reward_cfg
        self.command_cfg = command_cfg

        if not hasattr(gs, "_initialized") or not gs._initialized:
            gs.init(backend=gs.cpu)

        # ---- Scene setup (production visuals: outdoor theme) ----
        self.scene = gs.Scene(
            sim_options=gs.options.SimOptions(dt=self.dt, substeps=2),
            rigid_options=gs.options.RigidOptions(
                enable_self_collision=False,
            ),
            viewer_options=gs.options.ViewerOptions(
                camera_pos=(5.0, -3.0, 4.0),
                camera_lookat=(0.0, 0.0, 0.0),
                camera_fov=40,
                max_FPS=int(1.0 / self.dt),
            ),
            vis_options=gs.options.VisOptions(
                rendered_envs_idx=[0],
                lights=[
                    {"type": "directional", "dir": (-0.5, -0.3, -1.0), "color": (1.0, 0.95, 0.85), "intensity": 6.0},
                    {"type": "directional", "dir": (0.5, 0.5, -0.6), "color": (0.50, 0.60, 0.80), "intensity": 2.0},
                ],
                ambient_light=(0.30, 0.32, 0.35),
                background_color=(0.55, 0.70, 0.92),
                shadow=True,
            ),
            show_viewer=show_viewer,
        )

        # 1) Procedural terrain with mixed sub-terrain types
        self.terrain = self.scene.add_entity(
            gs.morphs.Terrain(
                n_subterrains=(3, 3),
                subterrain_size=(12.0, 12.0),
                horizontal_scale=0.25,
                vertical_scale=0.005,
                subterrain_types=[
                    ["flat_terrain", "random_uniform_terrain", "stepping_stones_terrain"],
                    ["pyramid_sloped_terrain", "discrete_obstacles_terrain", "wave_terrain"],
                    ["random_uniform_terrain", "pyramid_stairs_terrain", "sloped_terrain"],
                ],
                randomize=True,
            ),
            surface=gs.surfaces.Rough(color=(0.35, 0.55, 0.28)),
        )

        # 2) Robot (URDF)
        self.robot = self.scene.add_entity(
            gs.morphs.URDF(
                file="urdf/go2/urdf/go2.urdf",
                pos=env_cfg["base_init_pos"],
                quat=env_cfg["base_init_quat"],
            ),
            surface=gs.surfaces.Plastic(color=(0.18, 0.20, 0.24), roughness=0.35),
        )

        # 3) Mesh obstacles from STL assets (generated via cad_generate)
        self.obstacles = []
        for mesh_cfg in env_cfg.get("obstacle_meshes", []):
            mesh_file = mesh_cfg["file"]
            if Path(mesh_file).exists():
                entity = self.scene.add_entity(
                    gs.morphs.Mesh(
                        file=mesh_file,
                        pos=mesh_cfg.get("pos", (0, 0, 0)),
                        scale=mesh_cfg.get("scale", 0.001),
                        fixed=True,
                    ),
                    surface=gs.surfaces.Plastic(color=(0.48, 0.46, 0.42), roughness=0.6),
                )
                self.obstacles.append(entity)

        # Build scene AFTER all entities are added
        self.scene.build(n_envs=num_envs)

        # ---- Motor setup ----
        self.motors_dof_idx = torch.tensor(
            [self.robot.get_joint(name).dof_start for name in env_cfg["joint_names"]],
            dtype=gs.tc_int,
            device=gs.device,
        )
        self.robot.set_dofs_kp([env_cfg["kp"]] * self.num_actions, self.motors_dof_idx)
        self.robot.set_dofs_kv([env_cfg["kd"]] * self.num_actions, self.motors_dof_idx)

        # ---- Initial state ----
        self.global_gravity = torch.tensor([0.0, 0.0, -1.0], dtype=gs.tc_float, device=gs.device)
        self.init_base_pos = torch.tensor(env_cfg["base_init_pos"], dtype=gs.tc_float, device=gs.device)
        self.init_base_quat = torch.tensor(env_cfg["base_init_quat"], dtype=gs.tc_float, device=gs.device)
        self.inv_base_init_quat = inv_quat(self.init_base_quat)
        self.init_dof_pos = torch.tensor(
            [env_cfg["default_joint_angles"][j.name] for j in self.robot.joints[1:]],
            dtype=gs.tc_float,
            device=gs.device,
        )

        # ---- Buffers ----
        D, F = gs.device, gs.tc_float
        self.obs_buf = torch.empty((num_envs, self.num_obs), dtype=F, device=D)
        self.rew_buf = torch.empty((num_envs,), dtype=F, device=D)
        self.reset_buf = torch.ones((num_envs,), dtype=torch.bool, device=D)
        self.episode_length = torch.zeros((num_envs,), dtype=gs.tc_int, device=D)
        self.actions = torch.zeros((num_envs, self.num_actions), dtype=F, device=D)
        self.last_actions = torch.zeros_like(self.actions)
        self.commands = torch.zeros((num_envs, self.num_commands), dtype=F, device=D)
        self.base_lin_vel = torch.empty((num_envs, 3), dtype=F, device=D)
        self.base_ang_vel = torch.empty((num_envs, 3), dtype=F, device=D)
        self.projected_gravity = torch.empty((num_envs, 3), dtype=F, device=D)
        self.base_pos = torch.empty((num_envs, 3), dtype=F, device=D)
        self.extras = {}

        self.reward_scales = dict(reward_cfg["reward_scales"])
        self.obs_scales = obs_cfg["obs_scales"]

    def reset(self):
        """Reset all environments."""
        self.robot.set_qpos(
            torch.cat([self.init_base_pos, self.init_base_quat, self.init_dof_pos]).unsqueeze(0).expand(self.num_envs, -1)
        )
        self.robot.zero_all_dofs_velocity()
        self.episode_length.zero_()
        self.actions.zero_()
        self.last_actions.zero_()
        self._resample_commands(torch.arange(self.num_envs, device=self.device))
        self._compute_obs()
        return self.obs_buf, None

    def step(self, actions: torch.Tensor):
        """Step all environments."""
        self.last_actions[:] = self.actions
        self.actions[:] = torch.clip(actions, -1.0, 1.0)

        target = self.init_dof_pos + self.actions * self.env_cfg["action_scale"]
        self.robot.control_dofs_position(target, self.motors_dof_idx)

        self.scene.step()

        self._update_state()
        self._compute_obs()
        self._compute_rewards()

        self.episode_length += 1
        self.reset_buf = self.episode_length >= self.max_episode_length

        # Check fall termination
        base_z = self.base_pos[:, 2]
        self.reset_buf |= base_z < 0.15

        reset_ids = self.reset_buf.nonzero(as_tuple=False).flatten()
        if len(reset_ids) > 0:
            self._reset_envs(reset_ids)

        return self.obs_buf, self.rew_buf, self.reset_buf, self.extras

    def _update_state(self) -> None:
        """Read robot state from simulation."""
        self.base_pos[:] = self.robot.get_pos()
        base_quat = self.robot.get_quat()
        inv_quat_val = inv_quat(base_quat)
        self.base_lin_vel[:] = transform_by_quat(self.robot.get_vel(), inv_quat_val)
        self.base_ang_vel[:] = transform_by_quat(self.robot.get_ang(), inv_quat_val)
        self.projected_gravity[:] = transform_by_quat(
            self.global_gravity.expand(self.num_envs, -1), inv_quat_val
        )

    def _compute_obs(self) -> None:
        """Build observation tensor."""
        s = self.obs_scales
        self.obs_buf[:] = torch.cat([
            self.base_ang_vel * s["ang_vel"],
            self.projected_gravity,
            self.commands * torch.tensor([s["lin_vel"], s["lin_vel"], s["ang_vel"]], device=self.device),
            (self.robot.get_dofs_position(self.motors_dof_idx) - self.init_dof_pos) * s["dof_pos"],
            self.robot.get_dofs_velocity(self.motors_dof_idx) * s["dof_vel"],
            self.actions,
        ], dim=-1)

    def _compute_rewards(self) -> None:
        """Compute weighted reward sum from registry."""
        self.rew_buf.zero_()
        for name, fn in REWARD_REGISTRY.items():
            scale = self.reward_scales.get(name, 0.0)
            if scale != 0.0:
                self.rew_buf += scale * fn(self)

    def _resample_commands(self, env_ids: torch.Tensor) -> None:
        """Randomize velocity commands for given envs."""
        cfg = self.command_cfg
        n = len(env_ids)
        self.commands[env_ids, 0] = torch.empty(n, device=self.device).uniform_(*cfg["lin_vel_x_range"])
        self.commands[env_ids, 1] = torch.empty(n, device=self.device).uniform_(*cfg["lin_vel_y_range"])
        self.commands[env_ids, 2] = torch.empty(n, device=self.device).uniform_(*cfg["ang_vel_range"])

    def _reset_envs(self, env_ids: torch.Tensor) -> None:
        """Selective reset for terminated environments."""
        qpos = torch.cat([self.init_base_pos, self.init_base_quat, self.init_dof_pos])
        self.robot.set_qpos(qpos.unsqueeze(0).expand(len(env_ids), -1), env_ids)
        self.robot.zero_all_dofs_velocity(env_ids)
        self.episode_length[env_ids] = 0
        self.actions[env_ids] = 0
        self.last_actions[env_ids] = 0
        self._resample_commands(env_ids)
