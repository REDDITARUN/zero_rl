{"skill_usage": "Using trainer-config skill to generate training script.", "files": [{"path": "train.py", "content": "import argparse\nimport json\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Any, Deque, Dict\n\nfrom stable_baselines3 import A2C, DQN, PPO\nfrom stable_baselines3.common.base_class import BaseAlgorithm\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nfrom env import CollectorWorldEnv\n\n\nclass JsonLoggerCallback(BaseCallback):\n    '''Callback that streams JSON training metrics to stdout.'''\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.episode_count: int = 0\n        self.recent_rewards: Deque[float] = deque(maxlen=100)\n\n    def _on_step(self) -> bool:\n        infos: list[Dict[str, Any]] = self.locals.get(\"infos\", [])\n        for info in infos:\n            episode_data = info.get(\"episode\")\n            if episode_data:\n                reward = float(episode_data.get(\"r\", 0.0))\n                self.episode_count += 1\n                self.recent_rewards.append(reward)\n                payload = {\n                    \"type\": \"progress\",\n                    \"episode\": self.episode_count,\n                    \"reward\": reward,\n                    \"timesteps\": self.num_timesteps,\n                    \"avg_reward_100\": self._average_reward(),\n                }\n                print(json.dumps(payload), flush=True)\n        return True\n\n    def _average_reward(self) -> float:\n        if not self.recent_rewards:\n            return 0.0\n        return sum(self.recent_rewards) / len(self.recent_rewards)\n\n    @property\n    def avg_reward(self) -> float:\n        '''Return the current 100-episode rolling reward average.'''\n        return self._average_reward()\n\n\ndef parse_args() -> argparse.Namespace:\n    '''Parse CLI arguments for the training script.'''\n    parser = argparse.ArgumentParser(\n        description=\"Train CollectorWorldEnv with Stable-Baselines3 algorithms.\"\n    )\n    parser.add_argument(\n        \"--algorithm\",\n        type=str.lower,\n        choices=[\"ppo\", \"dqn\", \"a2c\"],\n        default=\"ppo\",\n        help=\"Algorithm to train (default: ppo).\",\n    )\n    parser.add_argument(\n        \"--timesteps\",\n        type=int,\n        default=10_000,\n        help=\"Total training timesteps (default: 10000).\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=3e-4,\n        help=\"Optimizer learning rate (default: 3e-4).\",\n    )\n    parser.add_argument(\n        \"--gamma\",\n        type=float,\n        default=0.99,\n        help=\"Discount factor (default: 0.99).\",\n    )\n    parser.add_argument(\n        \"--epsilon\",\n        type=float,\n        default=0.05,\n        help=\"Final epsilon for DQN exploration (default: 0.05).\",\n    )\n    parser.add_argument(\n        \"--n_steps\",\n        type=int,\n        default=2048,\n        help=\"Rollout steps per update (default: 2048).\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=64,\n        help=\"Batch size for updates (default: 64).\",\n    )\n    return parser.parse_args()\n\n\ndef build_env() -> DummyVecEnv:\n    '''Construct the monitored training environment.'''\n    return DummyVecEnv([lambda: Monitor(CollectorWorldEnv())])\n\n\ndef create_model(args: argparse.Namespace, env: DummyVecEnv) -> BaseAlgorithm:\n    '''Instantiate the requested algorithm with the provided parameters.'''\n    if args.algorithm == \"ppo\":\n        return PPO(\n            \"MlpPolicy\",\n            env,\n            learning_rate=args.learning_rate,\n            gamma=args.gamma,\n            n_steps=args.n_steps,\n            batch_size=args.batch_size,\n            verbose=0,\n        )\n    if args.algorithm == \"a2c\":\n        return A2C(\n            \"MlpPolicy\",\n            env,\n            learning_rate=args.learning_rate,\n            gamma=args.gamma,\n            n_steps=args.n_steps,\n            verbose=0,\n        )\n    buffer_size = max(args.n_steps * args.batch_size, args.batch_size)\n    return DQN(\n        \"MlpPolicy\",\n        env,\n        learning_rate=args.learning_rate,\n        gamma=args.gamma,\n        batch_size=args.batch_size,\n        buffer_size=buffer_size,\n        train_freq=1,\n        gradient_steps=1,\n        exploration_initial_eps=1.0,\n        exploration_final_eps=args.epsilon,\n        verbose=0,\n    )\n\n\ndef main() -> None:\n    '''Train the CollectorWorld agent and persist the resulting model.'''\n    args = parse_args()\n\n    validation_env = CollectorWorldEnv()\n    check_env(validation_env)\n    validation_env.close()\n\n    env = build_env()\n    callback = JsonLoggerCallback()\n    model = create_model(args, env)\n    model.learn(total_timesteps=args.timesteps, callback=callback)\n    env.close()\n\n    models_dir = Path(\"models\")\n    models_dir.mkdir(parents=True, exist_ok=True)\n    model_file = models_dir / f\"collectorworldenv_{args.algorithm}.zip\"\n    model.save(str(model_file))\n\n    completion_payload = {\n        \"type\": \"complete\",\n        \"summary\": {\n            \"algorithm\": args.algorithm.upper(),\n            \"timesteps\": args.timesteps,\n            \"episodes\": callback.episode_count,\n            \"avg_reward_100\": callback.avg_reward,\n            \"model_path\": str(model_file),\n        },\n    }\n    print(json.dumps(completion_payload), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}
