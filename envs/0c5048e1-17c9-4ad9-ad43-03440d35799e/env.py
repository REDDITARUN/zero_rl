"""ZeroCollectorEnv generated by ZeroRL."""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pygame
from typing import Any, Dict, Optional, Tuple


class ZeroCollectorEnv(gym.Env):
    """Simple grid navigation environment."""

    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(self, render_mode: Optional[str] = None, grid_size: int = 8, max_steps: int = 200):
        super().__init__()
        self.render_mode = render_mode
        self.grid_size = grid_size
        self.max_steps = max_steps

        self.action_space = spaces.Discrete(4)
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(6,), dtype=np.float32)

        self.window_size = 512
        self.window: Optional[pygame.Surface] = None
        self.clock: Optional[pygame.time.Clock] = None

        self.agent_pos = np.array([0, 0], dtype=np.int32)
        self.goal_pos = np.array([grid_size - 1, grid_size - 1], dtype=np.int32)
        self.steps = 0
        self.prev_distance = 0.0

    def reset(self, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None) -> Tuple[np.ndarray, Dict[str, Any]]:
        super().reset(seed=seed)
        self.agent_pos = np.array([0, 0], dtype=np.int32)
        self.goal_pos = np.array([self.grid_size - 1, self.grid_size - 1], dtype=np.int32)
        self.steps = 0
        self.prev_distance = self._distance_to_goal()

        obs = self._get_obs()
        info = self._get_info()
        if self.render_mode == "human":
            self._render_frame()
        return obs, info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        self.steps += 1
        self._apply_action(action)

        reward = self._compute_reward()
        terminated = bool(np.array_equal(self.agent_pos, self.goal_pos))
        truncated = self.steps >= self.max_steps

        obs = self._get_obs()
        info = self._get_info()
        if self.render_mode == "human":
            self._render_frame()
        return obs, float(reward), terminated, truncated, info

    def render(self) -> Optional[np.ndarray]:
        if self.render_mode == "rgb_array":
            return self._render_frame()
        return None

    def close(self) -> None:
        if self.window is not None:
            if self.render_mode == "human":
                pygame.display.quit()
            pygame.quit()
            self.window = None
            self.clock = None

    def _apply_action(self, action: int) -> None:
        move = np.array([0, 0], dtype=np.int32)
        if action == 0:
            move = np.array([-1, 0], dtype=np.int32)
        elif action == 1:
            move = np.array([0, 1], dtype=np.int32)
        elif action == 2:
            move = np.array([1, 0], dtype=np.int32)
        elif action == 3:
            move = np.array([0, -1], dtype=np.int32)

        nxt = self.agent_pos + move
        self.agent_pos = np.clip(nxt, 0, self.grid_size - 1)

    def _distance_to_goal(self) -> float:
        return float(np.linalg.norm(self.goal_pos.astype(np.float32) - self.agent_pos.astype(np.float32), ord=1))

    def _compute_reward(self) -> float:
        current_distance = self._distance_to_goal()
        reached_goal = np.array_equal(self.agent_pos, self.goal_pos)

        reward = -0.01
        reward += (self.prev_distance - current_distance) * 0.05
        if reached_goal:
            reward += 10.0
        self.prev_distance = current_distance
        return reward

    def _get_obs(self) -> np.ndarray:
        gx = max(float(self.grid_size - 1), 1.0)
        obs = np.array(
            [
                self.agent_pos[0] / gx,
                self.agent_pos[1] / gx,
                self.goal_pos[0] / gx,
                self.goal_pos[1] / gx,
                self._distance_to_goal() / (2.0 * gx),
                min(self.steps / float(self.max_steps), 1.0),
            ],
            dtype=np.float32,
        )
        return obs

    def _get_info(self) -> Dict[str, Any]:
        return {"steps": self.steps, "distance": self._distance_to_goal()}

    def _render_frame(self) -> Optional[np.ndarray]:
        if self.window is None:
            pygame.init()
            if self.render_mode == "human":
                pygame.display.init()
                self.window = pygame.display.set_mode((self.window_size, self.window_size))
            else:
                self.window = pygame.Surface((self.window_size, self.window_size))
            self.clock = pygame.time.Clock()

        canvas = pygame.Surface((self.window_size, self.window_size))
        canvas.fill((245, 240, 230))

        cell = self.window_size / self.grid_size
        for x in range(self.grid_size):
            for y in range(self.grid_size):
                rect = pygame.Rect(int(y * cell), int(x * cell), int(cell), int(cell))
                pygame.draw.rect(canvas, (210, 200, 185), rect, width=1)

        goal_rect = pygame.Rect(int(self.goal_pos[1] * cell), int(self.goal_pos[0] * cell), int(cell), int(cell))
        pygame.draw.rect(canvas, (103, 138, 99), goal_rect)

        center = (int((self.agent_pos[1] + 0.5) * cell), int((self.agent_pos[0] + 0.5) * cell))
        pygame.draw.circle(canvas, (65, 92, 87), center, int(cell * 0.25))

        if self.render_mode == "human" and self.window is not None:
            self.window.blit(canvas, canvas.get_rect())
            pygame.event.pump()
            pygame.display.update()
            if self.clock is not None:
                self.clock.tick(self.metadata["render_fps"])
            return None

        frame = np.transpose(np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2))
        return frame


def register_env() -> None:
    gym.register(
        id="ZeroCollector-v0",
        entry_point="env:ZeroCollectorEnv",
        max_episode_steps=200,
    )
